{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003538f1",
   "metadata": {},
   "source": [
    "# Disassembling the RealNVP Coupling Layer Formula\n",
    "\n",
    "This notebook demonstrates and explains the formula:\n",
    "\n",
    "$$\n",
    "y = x_m + (1 - \\text{mask}) \\cdot (x \\cdot \\exp(s_{out}) + t_{out})\n",
    "$$\n",
    "\n",
    "using a simple $3 \\times 3$ matrix example. Each step is shown with intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca64ce3f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "We will use PyTorch for matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7249ee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "print('PyTorch version:', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a369b",
   "metadata": {},
   "source": [
    "## 2. Define the Formula Components\n",
    "\n",
    "We will define the following tensors:\n",
    "- `x`: the input matrix\n",
    "- `mask`: the binary mask\n",
    "- `s_out`: the scale output\n",
    "- `t_out`: the translation output\n",
    "- `x_m`: the masked input\n",
    "\n",
    "Each will be a $3 \\times 3$ matrix for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5a7b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "mask =\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 0.]])\n",
      "x * mask (masked input) =\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 3. Create a Simple 3x3 Matrix Example\n",
    "\n",
    "# Input matrix x\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0],\n",
    "                  [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Binary mask: first two rows are 1, last row is 0\n",
    "mask = torch.tensor([[1, 1, 1],\n",
    "                    [1, 1, 1],\n",
    "                    [0, 0, 0]], dtype=torch.float32)\n",
    "\n",
    "# Show the result of applying the mask to x (masked input)\n",
    "x_m = x * mask\n",
    "print('x =\\n', x)\n",
    "print('mask =\\n', mask)\n",
    "print('x * mask (masked input) =\\n', x_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e5c9a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x_flat: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "--- RealNVP Coupling Layer Formula ---\n",
      "y = x_m + (1 - mask) * (x * exp(s_out) + t_out)\n",
      "  where:\n",
      "    x_m = x * mask\n",
      "    s_out, t_out = s_net(x_m), t_net(x_m)\n",
      "\n",
      "Input x: tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "Mask: tensor([1., 1., 1., 1., 1., 1., 0., 0., 0.])\n",
      "\n",
      "Step 1: x_m = x * mask\n",
      "  Formula: x_m = x * mask\n",
      "  Result: tensor([1., 2., 3., 4., 5., 6., 0., 0., 0.])\n",
      "\n",
      "Step 2: one_minus_mask = 1 - mask\n",
      "  Formula: one_minus_mask = 1 - mask\n",
      "  Result: tensor([0., 0., 0., 0., 0., 0., 1., 1., 1.])\n",
      "\n",
      "Step 3: s = s_net(x_m)\n",
      "  Formula: s = relu(fc1(x_m))\n",
      "  s after fc1+relu: [0.6064288020133972, 0.0, 1.403098225593567, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Formula: s = relu(fc2(s))\n",
      "  s after fc2+relu: [0.4582586884498596, 0.42421942949295044, 0.0, 0.00327242910861969, 0.0, 0.48316633701324463, 0.0, 0.014520853757858276, 0.0]\n",
      "  Formula: s = fc3(s)\n",
      "  s after fc3: [-0.30390143394470215, -0.40733546018600464, -0.2363838404417038, 0.28505009412765503, 0.3268551826477051, 0.14086377620697021, 0.19994595646858215, 0.08033043146133423, -0.298998087644577]\n",
      "  Formula: s = tanh(s)\n",
      "  s after tanh: [-0.29487890005111694, -0.3862079083919525, -0.2320772409439087, 0.2775725722312927, 0.3156921863555908, 0.13993941247463226, 0.19732338190078735, 0.08015808463096619, -0.29039546847343445]\n",
      "\n",
      "Step 4: t = t_net(x_m)\n",
      "  Formula: t = relu(fc1(x_m))\n",
      "  t after fc1+relu: [1.3482288122177124, 0.0, 0.021838873624801636, 0.0, 2.2258288860321045, 0.3676227927207947, 0.0, 2.390810489654541, 1.9952280521392822]\n",
      "  Formula: t = relu(fc2(t))\n",
      "  t after fc2+relu: [0.03677713871002197, 0.0, 0.0, 0.0, 0.0, 0.44672420620918274, 0.0, 0.7562161684036255, 0.0]\n",
      "  Formula: t = fc3(t)\n",
      "  t after fc3: [-0.2060638666152954, 0.07944735884666443, 0.12056563794612885, -0.02143137902021408, -0.3962918519973755, -0.33303695917129517, 0.19061991572380066, 0.06317491829395294, 0.16077560186386108]\n",
      "\n",
      "Step 5: x_exp_s = x * exp(s)\n",
      "  Formula: x_exp_s = x * exp(s)\n",
      "  Result: tensor([0.7446, 1.3593, 2.3787, 5.2797, 6.8560, 6.9012, 8.5270, 8.6677, 6.7317],\n",
      "       grad_fn=<MulBackward0>)\n",
      "\n",
      "Step 6: x_exp_s_plus_t = x_exp_s + t\n",
      "  Formula: x_exp_s_plus_t = x_exp_s + t\n",
      "  Result: tensor([0.5386, 1.4387, 2.4992, 5.2583, 6.4597, 6.5682, 8.7176, 8.7308, 6.8925],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Step 7: transformed = (1 - mask) * x_exp_s_plus_t\n",
      "  Formula: transformed = (1 - mask) * (x * exp(s) + t)\n",
      "  Result: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 8.7176, 8.7308, 6.8925],\n",
      "       grad_fn=<MulBackward0>)\n",
      "\n",
      "Step 8: y = x_m + transformed\n",
      "  Formula: y = x_m + transformed\n",
      "  Result: tensor([1.0000, 2.0000, 3.0000, 4.0000, 5.0000, 6.0000, 8.7176, 8.7308, 6.8925],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 6. RealNVP-style coupling layer: s and t networks in one forward function, with step-by-step y computation\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Flatten x to a vector (not masked)\n",
    "x_flat = x.view(-1)\n",
    "input_dim = x_flat.shape[0]\n",
    "output_dim = input_dim  # for demonstration\n",
    "\n",
    "class RealNVPCouplingLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, mask):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        # s_net (scale)\n",
    "        self.s_fc1 = nn.Linear(in_dim, out_dim)\n",
    "        self.s_fc2 = nn.Linear(out_dim, out_dim)\n",
    "        self.s_fc3 = nn.Linear(out_dim, out_dim)\n",
    "        # t_net (translation)\n",
    "        self.t_fc1 = nn.Linear(in_dim, out_dim)\n",
    "        self.t_fc2 = nn.Linear(out_dim, out_dim)\n",
    "        self.t_fc3 = nn.Linear(out_dim, out_dim)\n",
    "    def forward(self, x):\n",
    "        formula = (\n",
    "            \"y = x_m + (1 - mask) * (x * exp(s_out) + t_out)\\n\"\n",
    "            \"  where:\\n\"\n",
    "            \"    x_m = x * mask\\n\"\n",
    "            \"    s_out, t_out = s_net(x_m), t_net(x_m)\\n\"\n",
    "        )\n",
    "        print('--- RealNVP Coupling Layer Formula ---')\n",
    "        print(formula)\n",
    "        print('Input x:', x)\n",
    "        print('Mask:', self.mask)\n",
    "        # Step 1: Masked input\n",
    "        x_m = x * self.mask.view(-1)\n",
    "        print('\\nStep 1: x_m = x * mask')\n",
    "        print('  Formula: x_m = x * mask')\n",
    "        print('  Result:', x_m)\n",
    "        # Step 2: 1 - mask\n",
    "        one_minus_mask = 1 - self.mask.view(-1)\n",
    "        print('\\nStep 2: one_minus_mask = 1 - mask')\n",
    "        print('  Formula: one_minus_mask = 1 - mask')\n",
    "        print('  Result:', one_minus_mask)\n",
    "        # Step 3: s_net(x_m)\n",
    "        s = F.relu(self.s_fc1(x_m))\n",
    "        print('\\nStep 3: s = s_net(x_m)')\n",
    "        print('  Formula: s = relu(fc1(x_m))')\n",
    "        print('  s after fc1+relu:', s.tolist())\n",
    "        s = F.relu(self.s_fc2(s))\n",
    "        print('  Formula: s = relu(fc2(s))')\n",
    "        print('  s after fc2+relu:', s.tolist())\n",
    "        s = self.s_fc3(s)\n",
    "        print('  Formula: s = fc3(s)')\n",
    "        print('  s after fc3:', s.tolist())\n",
    "        s = torch.tanh(s)\n",
    "        print('  Formula: s = tanh(s)')\n",
    "        print('  s after tanh:', s.tolist())\n",
    "        # Step 4: t_net(x_m)\n",
    "        t = F.relu(self.t_fc1(x_m))\n",
    "        print('\\nStep 4: t = t_net(x_m)')\n",
    "        print('  Formula: t = relu(fc1(x_m))')\n",
    "        print('  t after fc1+relu:', t.tolist())\n",
    "        t = F.relu(self.t_fc2(t))\n",
    "        print('  Formula: t = relu(fc2(t))')\n",
    "        print('  t after fc2+relu:', t.tolist())\n",
    "        t = self.t_fc3(t)\n",
    "        print('  Formula: t = fc3(t)')\n",
    "        print('  t after fc3:', t.tolist())\n",
    "        # Step 5: x * exp(s)\n",
    "        x_exp_s = x * torch.exp(s)\n",
    "        print('\\nStep 5: x_exp_s = x * exp(s)')\n",
    "        print('  Formula: x_exp_s = x * exp(s)')\n",
    "        print('  Result:', x_exp_s)\n",
    "        # Step 6: x * exp(s) + t\n",
    "        x_exp_s_plus_t = x_exp_s + t\n",
    "        print('\\nStep 6: x_exp_s_plus_t = x_exp_s + t')\n",
    "        print('  Formula: x_exp_s_plus_t = x_exp_s + t')\n",
    "        print('  Result:', x_exp_s_plus_t)\n",
    "        # Step 7: (1 - mask) * [x * exp(s) + t]\n",
    "        transformed = one_minus_mask * x_exp_s_plus_t\n",
    "        print('\\nStep 7: transformed = (1 - mask) * x_exp_s_plus_t')\n",
    "        print('  Formula: transformed = (1 - mask) * (x * exp(s) + t)')\n",
    "        print('  Result:', transformed)\n",
    "        # Step 8: y = x_m + transformed\n",
    "        y = x_m + transformed\n",
    "        print('\\nStep 8: y = x_m + transformed')\n",
    "        print('  Formula: y = x_m + transformed')\n",
    "        print('  Result:', y)\n",
    "        return y, s, t\n",
    "\n",
    "# Instantiate the coupling layer with the mask (flattened to match x_flat)\n",
    "coupling = RealNVPCouplingLayer(input_dim, output_dim, mask.view(-1))\n",
    "\n",
    "# Pass x_flat (not masked) through the coupling layer\n",
    "print('Input x_flat:', x_flat.tolist())\n",
    "y, s_out, t_out = coupling(x_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fd199",
   "metadata": {},
   "source": [
    "## Step-by-step Explanation of the RealNVP Coupling Layer Formula\n",
    "\n",
    "The RealNVP coupling layer formula is:\n",
    "\n",
    "$$\n",
    "y = x_m + (1 - \\text{mask}) \\cdot (x \\cdot \\exp(s_{out}) + t_{out})\n",
    "$$\n",
    "\n",
    "- **$x$**: The original input vector (flattened from the matrix).\n",
    "- **$\\text{mask}$**: A binary mask (1 = masked, 0 = unmasked).\n",
    "- **$x_m = x * \\text{mask}$**: The masked input, where only the masked (unchanged) parts of $x$ are kept, the rest are set to zero.\n",
    "- **$s_{out}, t_{out}$**: The outputs of the scale and translation networks, computed from $x_m$ (the masked input).\n",
    "- **$x \\cdot \\exp(s_{out}) + t_{out}$**: The network's transformation, applied to all elements of $x$.\n",
    "- **$(1 - \\text{mask})$**: Selects only the unmasked part (where mask = 0).\n",
    "- **$x_m + (1 - \\text{mask}) \\cdot (\\ldots)$**: The final output $y$ is constructed by:\n",
    "    - Copying the masked part (mask = 1) from $x$ (unchanged)\n",
    "    - Replacing the unmasked part (mask = 0) with the network's prediction\n",
    "\n",
    "**In summary:**\n",
    "- The network predicts only the unmasked part, conditioned on the masked part.\n",
    "- The masked part is copied directly from the input.\n",
    "- This ensures invertibility and efficient computation in RealNVP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229860d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ef0e504",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
